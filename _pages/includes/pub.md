
# üìù Publications 
## ‚è±Ô∏é Efficient DMs


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/fs.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Post-training Quantization with Progressive Calibration and Activation Relaxing for Text-to-Image Diffusion Models](https://arxiv.org/pdf/2311.06322) \\
**Siao Tang**, Xin Wang, Hong Chen, Chaoyu Guan, Zewen Wu, Yansong Tang, Wenwu Zhu.

<!-- [**Project**](https://speechresearch.github.io/fastspeech/) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong> -->

<!-- - FastSpeech is the first fully parallel end-to-end speech synthesis model.
- **Academic Impact**: This work is included by many famous speech synthesis open-source projects, such as [ESPNet ![](https://img.shields.io/github/stars/espnet/espnet?style=social)](https://github.com/espnet/espnet). Our work are promoted by more than 20 media and forums, such as [Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s/UkFadiUBy-Ymn-zhJ95JcQ)„ÄÅ[InfoQ](https://www.infoq.cn/article/tvy7hnin8bjvlm6g0myu).
- **Industry Impact**: FastSpeech has been deployed in [Microsoft Azure TTS service](https://techcommunity.microsoft.com/t5/azure-ai/neural-text-to-speech-extends-support-to-15-more-languages-with/ba-p/1505911) and supports 49 more languages with state-of-the-art AI quality. It was also shown as a text-to-speech system acceleration example in [NVIDIA GTC2020](https://resources.nvidia.com/events/GTC2020s21420). -->
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI, 2024</div><img src='images/fs.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Disentangled representation learning](https://arxiv.org/pdf/2211.11695) \\
Xin Wang, Hong Chen, **Siao Tang**, Zihao Wu, Wenwu Zhu.

</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='images/fs.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Lightweight Diffusion Models with Distillation-Based Block Neural Architecture Search](https://arxiv.org/pdf/2311.04950) \\
**Siao Tang**, Xin Wang, Hong Chen, Chaoyu Guan, Yansong Tang, Wenwu Zhu.

</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='images/fs.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DisenStudio: Customized Multi-subject Text-to-Video Generation with Disentangled Spatial Control](https://arxiv.org/pdf/2405.12796) \\
Hong Chen, Xin Wang, Yipeng Zhang, Yuwei Zhou, Zeyang Zhang, **Siao Tang**, Wenwu Zhu.

</div>
</div>





<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='images/fs.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Commonsense Learning: An Indispensable Path towards Human-centric Multimedia](https://mn.cs.tsinghua.edu.cn/xinwang/PDF/papers/2020_Commonsense%20Learning%20An%20Indispensable%20Path%20towards%20Human-centric%20Multimedia.pdf) \\
Bin Huang\*, **Siao Tang\***, Guangyao Shen, Guohao Li, Xin Wang, Wenwu Zhu.

</div>
</div>









